\section{Integrated Gradients with Captum for Explainable Entity Matching}

\subsection{Captum: A Framework for Model Interpretability}

Captum is an open-source interpretability library developed by Facebook AI Research for PyTorch-based models \cite{kokhlikyan2020captum}. It provides a unified interface for a wide range of feature attribution and explanation methods. Captum is designed to be model-agnostic within the PyTorch ecosystem and supports both computer vision and natural language processing (NLP) tasks.

A key advantage of Captum is its native support for \emph{Integrated Gradients (IG)}, a theoretically grounded attribution method that satisfies desirable axiomatic properties such as sensitivity and completeness \cite{sundararajan2017axiomatic}. For NLP applications, Captum allows IG to be computed directly in the \emph{embedding space}, which is crucial for models that operate on dense vector representations rather than discrete tokens. This makes Captum particularly suitable for explainability in entity matching systems based on deep neural architectures such as Transformers or Siamese networks.

\subsection{Integrated Gradients: Definition and Intuition}

Integrated Gradients attributes the prediction of a model to its input features by integrating the gradients of the model’s output with respect to the input along a straight-line path from a \emph{baseline} $x'$ to the actual input $x$. Formally, for a model $F$ and input feature $x_i$, the Integrated Gradient is defined as:

\begin{equation}
\mathrm{IG}_i(x) = (x_i - x'_i) \cdot \int_{\alpha=0}^{1} 
\frac{\partial F\big(x' + \alpha (x - x')\big)}{\partial x_i} \, d\alpha
\end{equation}

where:
\begin{itemize}
    \item $x$ denotes the actual input,
    \item $x'$ denotes the baseline input,
    \item $\alpha$ interpolates between baseline and input,
    \item $\frac{\partial F}{\partial x_i}$ is the gradient of the model output with respect to the $i$-th input feature.
\end{itemize}

The resulting attribution score quantifies how much each feature contributes to the change in the model’s prediction from the baseline to the input.

\subsection{The Role of the Baseline $x'$}

Integrated Gradients explains \emph{the difference in prediction between the baseline and the input}, rather than the absolute prediction itself. Consequently, the choice of baseline is critical and directly affects the semantic validity of the explanation.

\paragraph{Relative Zero Point}

The baseline represents a \emph{relative zero point} for attribution. It must not convey meaningful information or semantic context. A naïve choice such as a zero vector (null matrix) can be problematic, especially in NLP. If such values were never observed during training, the model may interpret them as out-of-distribution noise rather than the absence of information, leading to distorted attributions \cite{kindermans2019reliability}.

\paragraph{Information vs. Absence of Information}

An effective baseline should encode \emph{informational absence}, not alternative information. In NLP models, embeddings are learned representations with structured geometry; therefore, a zero embedding does not necessarily correspond to ``no token.'' Instead, padding tokens such as \texttt{[PAD]}, which are explicitly introduced during training and masked appropriately, more reliably represent the absence of semantic content \cite{wallace2019trick}.

\subsection{Saturation and the Need for Integration}

A central motivation for Integrated Gradients is the \emph{saturation problem} of standard gradient-based explanations. When a model is highly confident in its prediction (e.g., output probability close to $0.99$), the gradient at the input $x$ may be near zero due to local flatness of the decision function. Relying solely on this gradient would incorrectly suggest that the input features are unimportant.

Integrated Gradients mitigates this issue by accumulating gradients \emph{along the entire path} from the baseline (where the prediction may be low, e.g., $0.1$) to the final input. This captures how the model’s confidence evolves throughout the input space, providing a faithful attribution even in saturated regions of the function \cite{sundararajan2017axiomatic}.

\subsection{Practical Implementation in Captum}

In practice, the path integral is approximated using a finite Riemann sum over $m$ discrete steps:

\begin{equation}
\mathrm{IG}_i(x) \approx (x_i - x'_i) \cdot \frac{1}{m} 
\sum_{k=1}^{m} 
\frac{\partial F\big(x' + \frac{k}{m}(x - x')\big)}{\partial x_i}
\end{equation}

Captum implements this approximation efficiently and allows practitioners to control the number of integration steps, balancing computational cost and attribution stability.

In the experiments for this paper, we used \(m = 200\) steps.

\subsection{Baseline Evaluation for Integrated Gradients in Entity Matching}

The choice of baseline is a critical design decision for Integrated Gradients, particularly in NLP tasks where inputs are embedded in a high-dimensional semantic space. To empirically assess the impact of different baselines on explanation quality, we compare three commonly used baselines: a zero vector baseline (\texttt{ZERO}), a padding token baseline (\texttt{[PAD]}), and a masking token baseline (\texttt{[MASK]}). For the latter two, token identifiers are obtained using the AutoTokenizer from the pretrained BERT-Model, specifically \texttt{tokenizer.pad\_token\_id} and \texttt{tokenizer.mask\_token\_id}.

All baselines are evaluated in the embedding space, ensuring compatibility with the model’s learned representation geometry. The goal of this evaluation is not to optimize predictive performance, but to analyze the \emph{stability}, \emph{faithfulness}, and \emph{structural bias} of the resulting attributions.

\subsubsection{Evaluation Metrics}

We assess baseline quality using three complementary metrics:

\paragraph{Average Convergence Delta}
Integrated Gradients theoretically satisfies the Completeness Axiom, which implies that the sum of all feature attributions should equal $F(x) - F(x')$. In practice, numerical approximation introduces a convergence error. Captum reports this error as the \emph{convergence delta}. We compute the average convergence delta across the evaluation set to measure how well each baseline supports faithful numerical integration. Lower values indicate better adherence to the completeness property and more reliable attributions.

\paragraph{Average Standard Deviation}
To quantify explanation stability, we compute the average standard deviation of attribution scores across multiple inputs. High variance suggests sensitivity to minor input changes or numerical artifacts, whereas low variance indicates stable and robust explanations. This metric captures the extent to which a baseline produces consistent attributions across different entity pairs.

\paragraph{Structural Bias}
Structural tokens such as \texttt{[CLS]} and \texttt{[SEP]} are required by Transformer architectures but typically do not carry semantic information relevant to entity matching decisions. We therefore measure \emph{structural bias} as the percentage of total attribution mass assigned to these tokens. Lower structural bias indicates that the explanation focuses on semantically meaningful tokens rather than architectural artifacts.

\subsubsection{Results and Discussion}

Across all three metrics, the \texttt{[PAD]} and \texttt{[MASK]} baselines consistently outperform the zero baseline. Both token-based baselines exhibit significantly lower convergence deltas, indicating improved numerical faithfulness with respect to the Completeness Axiom. Furthermore, they yield lower attribution variance, suggesting more stable explanations.

The zero baseline performs worst across all metrics, supporting the hypothesis that a null embedding does not represent true informational absence and may instead introduce out-of-distribution noise. This aligns with prior findings that inappropriate baselines can lead to unreliable saliency maps in gradient-based explanation methods \cite{kindermans2019reliability}.

Between the token-based baselines, \texttt{[MASK]} performs slightly better than \texttt{[PAD]}. In particular, it achieves the lowest average convergence delta and the lowest structural bias, assigning a smaller proportion of attribution mass to \texttt{[CLS]} and \texttt{[SEP]}. A plausible explanation is that \texttt{[MASK]} embeddings are explicitly used during pretraining to represent missing information, making them a more semantically faithful approximation of informational absence than padding tokens, which primarily serve alignment purposes.

\subsubsection{Implications for Explainable Entity Matching}

These results provide empirical evidence that baseline selection substantially affects the quality of Integrated Gradients explanations in entity matching tasks. Token-based baselines that are part of the model’s training distribution yield more faithful, stable, and semantically grounded attributions than a zero vector baseline. The superior performance of the \texttt{[MASK]} baseline suggests that baselines designed to model missing information during pretraining are particularly well-suited for attribution methods in Transformer-based NLP models.

Consequently, we adopt \texttt{[MASK]} as the default baseline for all subsequent explanation analyses in this work.
