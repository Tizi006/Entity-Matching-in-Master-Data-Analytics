\section{Conclusion}
In this project, we investigated explainable entity matching by applying black-box explainers such as SHAP, LIME, and related methods to a fixed EM model. 
We ran a set of benchmark EM datasets through a black-box model, 
generated local explanations for its duplicate and non-duplicate predictions, and compared these explanation methods with respect to fidelity, 
stability, and usability. We further visualized the explanations to make the decision process of the EM model accessible to human users. 
The overall objective was to implement and evaluate interpretable methods that clearly explain why two records are classified as duplicates or not, 
thereby increasing trust in EM systems and supporting users in debugging, validating, and improving data integration pipelines.