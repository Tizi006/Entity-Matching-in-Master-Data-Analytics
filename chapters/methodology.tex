\section{Methodology}
Our methodology follows a comparative framework designed to 
evaluate three distinct interpretability paradigms: gradient-based 
attribution, local linear surrogates, and counterfactual 
perturbations. This section details our model architecture, 
data serialization, and the technical implementation of our 
explanation modules.

\subsection{Dataset and Input Serialization}
We utilize the structured \textbf{DBLP-ACM} benchmark dataset 
from the Magellan repository \cite{li2020ditto}. The dataset 
comprises bibliographic records with four attributes: 
\textit{Title}, \textit{Authors}, \textit{Venue}, and \textit{Year}.



To process these records with a Transformer architecture, we 
serialize each entity pair $(e_a, e_b)$ into a single sequence. 
Attributes are prefixed with their labels and concatenated using 
the \texttt{[SEP]} token:
\begin{center}
\small \texttt{[CLS] Title: A [SEP] Title: B ... [SEP]}
\end{center}
This format allows the model's self-attention mechanism to 
perform cross-attribute comparison directly within the hidden layers.

\subsection{Black-Box Model: Transformer-based EMModel}
Our primary matching engine is a custom PyTorch-based 
\textit{EMModel}. The architecture leverages the 
\textit{bert-base-uncased} backbone \cite{devlin2019bert} 
containing 12 layers and 110M parameters.



The model processes the serialized input and extracts the 
final hidden state of the \texttt{[CLS]} token, denoted 
as $h_{[CLS]} \in \mathbf{text}{R}^{768}$, which serves as the 
aggregate semantic representation of the record pair. This 
vector is passed through a linear dropout layer and a 
classification head:
\[ P(\text{match}) = \text{Softmax}(W \cdot h_{[CLS]} + b) \]
where $W$ and $b$ are the weights and biases of the final layer.

\subsection{Feature Attribution: Integrated Gradients}
We implement \textbf{Integrated Gradients (IG)} \cite{sundararajan2017axiomatic} 
via the \textbf{Captum} library \cite{kokhlikyan2020captum}. IG 
calculates the integral of gradients along a straight-line path 
from a baseline $x'$ to the input $x$. In our implementation, we 
apply IG directly to the word embedding layer. This produces 
continuous importance scores that satisfy the axioms of 
\textit{completeness} and \textit{sensitivity}, revealing which 
specific tokens drive the matching decision.

\subsection{Local Linear Surrogates: LIME}
As a model-agnostic baseline, we employ \textbf{LIME} \cite{lime}. 
Unlike IG, which uses internal model gradients, LIME treats 
the \textit{EMModel} as a true black box. It generates 
interpretations by randomly perturbing the input tokens and 
training a local ridge regression model to approximate the 
decision boundary. This provides a comparison between 
gradient-derived and perturbation-derived importance.

\subsection{Counterfactuals: TextAttack Framework}
To provide actionable "what-if" insights, we generate 
counterfactual explanations using the \textbf{TextAttack} 
framework \cite{morris2020textattack}. We utilize the 
\textbf{TextFooler} algorithm \cite{textfooler2020} to perform 
minimal textual substitutions that flip the model's prediction 
(e.g., from "Match" to "Non-Match"). 



We enforce a \textbf{span-based constraint}: the master record 
from the DBLP source remains fixed, while only the ACM target 
record is mutable. This ensures the counterfactual is 
interpretable for data cleaning workflows, identifying 
the exact attribute changes required to alter the matching output.

\subsection{Comparative Objectives}
Our evaluation compares these methods based on:
\begin{itemize}
    \item \textbf{Fidelity:} The accuracy with which the explanation 
    represents the model's sensitivity.
    \item \textbf{Stability:} The consistency of the results 
    across similar record pairs.
    \item \textbf{Usability:} The clarity of the output for 
    human-in-the-loop validation.
\end{itemize}