\section{Related Work}
The field of Entity Resolution (ER) has evolved from 
foundational probabilistic frameworks to complex 
learning-based approaches. Recent research by 
Ravikanth et al. \cite{Ravikanth2024} emphasizes the 
need for efficient automatic record deduplication 
in large-scale datasets. Parallel to purely 
statistical models, structural approaches utilizing 
Matching Dependencies (MDs) have gained traction. 
Koumarelas et al. \cite{Koumarelas2020} introduced 
MDedup, leveraging dependencies for duplicate 
detection, while ERBlox \cite{Bahmani2016} demonstrated 
how machine learning can be combined with MDs to 
improve both accuracy and scalability.



The development of high-performance black-box models 
in ER has followed the broader trends in Deep Learning. 
Early deep models, such as \textbf{DeepMatcher} \cite{deepmatcher}, 
formalized the use of bidirectional Gated Recurrent Units 
(GRUs) and Siamese architectures to summarize attribute 
information. These models focus on learning vector 
representations for each record independently before 
comparing them in a unified classification space. 

With the advent of the Transformer architecture, the 
state-of-the-art transitioned toward Pre-trained 
Language Models (PLMs). Architectures like \textbf{Ditto} 
\cite{li2020ditto} utilize BERT-based Cross-Encoders 
to perform deep, token-level comparisons. Unlike Siamese 
models, these "all-to-all" attention mechanisms allow 
the model to capture complex semantic interactions 
between record pairs at every layer. While these 
advancements have led to superior F1-scores, they 
have also increased the opacity of the matching 
logic, necessitating the interpretability 
approaches explored in this work.



As deep learning models became the standard, the 
focus shifted toward transparency and fairness. 
Ara√∫jo et al. \cite{Araujo2025} provided a 
comprehensive overview of the current state of 
fairness and explanations in ER, highlighting 
ethical and technical requirements for trustworthy 
systems. Barlaug \cite{Barlaug2023} introduced 
\textbf{LEMON}, a framework specifically designed 
for explainable entity matching that bridges the 
gap between black-box performance and human 
interpretability. Similarly, Baraldi et al. \cite{Baraldi2023} 
proposed intrinsically interpretable EM systems, 
arguing that transparency should be embedded 
within the model architecture itself rather than 
added post-hoc.

A significant challenge in explaining ER decisions 
is the "verbosity" of token-level attributions. 
Benassi et al. \cite{Benassi2024} addressed this by 
proposing word clustering to group semantic 
information, thereby making explanations more 
accessible to data stewards. This move toward 
more refined, semantic-level explanation aligns 
with our objective of providing usable insights 
into transformer-based matching decisions.

Regarding instance-based interpretability, 
\textbf{Counterfactual Explanations} represent a 
key development. Wang and Li \cite{Wang2022Minun} 
introduced \textbf{Minun}, a system dedicated to 
evaluating counterfactuals specifically for EM tasks. 
Their work explores how minimal changes in record 
attributes can be used to probe model behavior. 
Our research extends these efforts by utilizing an 
adversarial search paradigm within the TextAttack 
framework. By combining the theoretical foundations 
of counterfactual evaluation with a span-based 
locality constraint, we address the practical 
needs of master data management where incoming 
data must be validated against a fixed reference.

\pagebreak