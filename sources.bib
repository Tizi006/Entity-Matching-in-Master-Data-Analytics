% --- Foundational & Structural Entity Matching ---

@article{fellegi1969,
  author  = {Fellegi, Ivan P. and Sunter, Alan B.},
  journal = {Journal of the American Statistical Association},
  title   = {A Theory for Record Linkage},
  volume  = {64},
  number  = {328},
  pages   = {1183--1210},
  year    = {1969}
}

@inproceedings{deepmatcher,
  author    = {Mudgal, Sidharth and others},
  title     = {Deep Learning for Entity Matching: A Design Space Exploration},
  booktitle = {Proceedings of SIGMOD},
  year      = {2018},
  doi       = {10.1145/3183713.3196926}
}

@inproceedings{li2020ditto,
  author    = {Li, Yuliang and others},
  title     = {Deep Entity Matching with Pre-trained Language Models},
  booktitle = {Proceedings of SIGMOD},
  year      = {2020}
}

@article{Koumarelas2020,
  author  = {Koumarelas, I. and Papenbrock, T. and Naumann, F.},
  title   = {MDedup: Duplicate Detection with Matching Dependencies},
  journal = {Proc. VLDB Endow.},
  volume  = {13},
  number  = {5},
  year    = {2020}
}

@misc{Bahmani2016,
  author       = {Bahmani, Z. and others},
  title        = {ERBlox: Combining Matching Dependencies with Machine Learning},
  howpublished = {CoRR, arXiv:1602.02334},
  year         = {2016}
}

@article{Ravikanth2024,
  author  = {Ravikanth, M. and others},
  title   = {An Efficient Learning-Based Approach for Automatic Record Deduplication},
  journal = {Scientific Reports},
  volume  = {14},
  year    = {2024}
}

% --- Transformers & NLP ---

@inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and others},
  booktitle = {Advances in NeurIPS},
  title     = {Attention is All You Need},
  year      = {2017}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@inproceedings{wolf2020transformers,
  author    = {Wolf, Thomas and others},
  booktitle = {Proceedings of EMNLP: System Demonstrations},
  title     = {Transformers: State-of-the-Art NLP},
  year      = {2020}
}

% --- XAI Methods & Frameworks ---

@inproceedings{lime,
  author    = {Ribeiro, Marco Tulio and others},
  booktitle = {Proceedings of ACM SIGKDD},
  title     = {"Why Should I Trust You?": Explaining Predictions},
  year      = {2016}
}

@article{Barlaug2023,
  author  = {Barlaug, Niklas and Gulla, Jon Atle},
  title   = {LEMON: Explainable Entity Matching},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume  = {35},
  number  = {8},
  pages   = {8171--8184},
  year    = {2023},
  doi     = {10.1109/TKDE.2022.3200644}
}

@inproceedings{shap,
  author    = {Lundberg, Scott M. and Lee, Su-In},
  booktitle = {Advances in NeurIPS},
  title     = {A Unified Approach to Interpreting Model Predictions},
  year      = {2017}
}

@article{wachter2017,
  author  = {Wachter, Sandra and others},
  journal = {Harvard Journal of Law \& Technology},
  title   = {Counterfactual Explanations without Opening the Black Box},
  year    = {2017},
  volume  = {31}
}

@inproceedings{sundararajan2017axiomatic,
  author    = {Sundararajan, Mukund and others},
  title     = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of ICML},
  year      = {2017}
}

@article{kokhlikyan2020captum,
  author  = {Kokhlikyan, Narine and others},
  title   = {Captum: A unified and generic model interpretability library for PyTorch},
  journal = {arXiv preprint arXiv:2009.07896},
  year    = {2020}
}

% --- Adversarial NLP (Counterfactuals) ---

@inproceedings{morris2020textattack,
  author    = {Morris, John and others},
  title     = {TextAttack: A Framework for Adversarial Attacks},
  booktitle = {Proceedings of EMNLP: System Demonstrations},
  year      = {2020}
}

@inproceedings{textfooler2020,
  author    = {Jin, Di and others},
  title     = {Is BERT Really Robust? A Strong Baseline for NLP Attack},
  booktitle = {Proceedings of AAAI},
  year      = {2020}
}

% --- Domain-Specific EM XAI ---

@article{Araujo2025,
  author  = {Ara√∫jo, T. and others},
  title   = {Fairness and Explanations in Entity Resolution: An Overview},
  journal = {IEEE Access},
  year    = {2025},
  doi     = {10.1109/ACCESS.2025.3599990}
}

@article{lemon,
  author  = {Barlaug, N.},
  title   = {LEMON: Explainable Entity Matching},
  journal = {IEEE TKDE},
  volume  = {35},
  year    = {2023}
}

@inproceedings{exmatchina2021,
  author    = {Singh, Kushagra and others},
  title     = {ExMatchina: Explainable Entity Matching},
  booktitle = {Proceedings of SIGMOD},
  year      = {2021}
}

@inproceedings{thirumuruganathan2022,
  author    = {Thirumuruganathan, Saravanan and others},
  title     = {Explaining Deep Entity Matching Decisions},
  booktitle = {Proceedings of SIGMOD},
  year      = {2022}
}

@inproceedings{Wang2022Minun,
  author    = {Wang, J. and Li, Y.},
  title     = {Minun: Evaluating Counterfactual Explanations for Entity Matching},
  booktitle = {Proceedings of the DEEM Workshop},
  year      = {2022}
}

@inproceedings{Baraldi2023,
  author    = {Baraldi, A. and others},
  title     = {An Intrinsically Interpretable Entity Matching System},
  booktitle = {Proceedings of EDBT},
  year      = {2023}
}

@inproceedings{Benassi2024,
  author    = {Benassi, R. and others},
  title     = {Explaining Entity Matching with Clusters of Words},
  booktitle = {Proceedings of IEEE ICDE},
  year      = {2024}
}

% --- Saliency & Reliability ---

@inproceedings{kindermans2019reliability,
  author    = {Kindermans, Pieter-Jan and others},
  title     = {The (Un)Reliability of Saliency Methods},
  booktitle = {Advances in NeurIPS},
  year      = {2019}
}

@inproceedings{wallace2019trick,
  author    = {Wallace, Eric and others},
  title     = {Trick Me If You Can: Adversarial Examples},
  booktitle = {Proceedings of ACL},
  year      = {2019}
}